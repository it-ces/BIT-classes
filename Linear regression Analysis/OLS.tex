\documentclass{beamer}
\usetheme{Madrid}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{nicematrix}
\usepackage{tikz}
\usepackage{comment}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{array}
\usepackage{pgfcore}
\usepackage{listings}
\usepackage{color}
\definecolor{munsell}{rgb}{0.0, 0.5, 0.69}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{minas}{RGB}{0.244, 0.172, 0.36}
\definecolor{PUJ}{RGB}{44, 86, 151}
\definecolor{PUJ2}{RGB}{43, 93, 156}
\definecolor{PUJ3}{RGB}{20, 52, 107}
\definecolor{cyandk}{rgb}{0.0, 0.72, 0.92}
\setbeamerfont{frametitle}{size=\LARGE ,series=\bfseries}
\setbeamercolor{frametitle}{fg=PUJ2, bg=white} %% title of the beamer
\setbeamercolor{titlelike}
{parent=structure,bg=PUJ2}
\setbeamercolor{title}{fg=white, bg=PUJ3} 
%\setbeamercolor{navigation symbols}{fg=white, bg=white}
\setbeamercolor*{palette primary}{use=structure,fg=black,bg=yellow}
\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=PUJ3}
\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=PUJ3}

\setbeamercolor{block title}{bg=PUJ3,fg=white}


%% code information
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  classoffset=1,
  morekeywords={True,False}, keywordstyle=\color{munsell}, 
  classoffset=0, 
  keywordstyle=\color{blue},  
  commentstyle=\color{dkgreen},
  stringstyle=\color{PUJ3},
  numberstyle=\tiny\color{gray},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4,
}

%% You can change default language in the middle of document with \lstset{language=Java}.


%% PUT or Remove the logo in a slide.


%% Information topic

\institute{Javeriana}
\date{2020}

\title[Pontificia Universidad Javeriana] %optional
{Linear Regression Analysis}
\subtitle{ using python.}

\author[Iván Andrés Trujillo] 
{
Iván Andrés Trujilllo Abella}

\institute[] 
{
  Facultad de Ingenieria\\
  Pontificia Universidad Javeriana
  \and
  
\textbf{ trujilloiv@javeriana.edu.co \\
addajaveriana}
}

\date[MINTA] % (optional)

\newif\ifplacelogo % create a new conditional
\placelogotrue % set it to true
%\logo{\ifplacelogo\color{red}\rule{.5cm}{.5cm}\fi}
\logo{\ifplacelogo \includegraphics[height= 2.0cm]{pujshield.eps}\fi}



\begin{document}



\frame{\titlepage}


\begin{frame}{Ordinary least squares}

\begin{equation}
u = \sum (y_{i} - \hat{y_{i}})^{2}
\end{equation}
note that $\hat{y_{i}} = \beta_{0} + \beta_{1} x_{i}$.

\begin{equation}
u^{2} = \sum (y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}
\end{equation}

the first order condition require $\frac{\partial u^{2}}{\partial \beta_{0}}=0,\frac{\partial u^{2}}{\partial \beta_{1}}=0$.
\end{frame}


\begin{frame}{Chain rule}
to get 
\begin{equation}
\frac{\partial u^{2}}{ \partial \beta_{0}} = \frac{\partial u^{2}}{\partial u}\frac{\partial u}{\partial \beta_{0}} 
\end{equation}


\begin{equation}
\frac{\partial u^{2}}{ \partial \beta_{1}} = \frac{\partial u^{2}}{\partial u}\frac{\partial u}{\partial \beta_{1}} 
\end{equation}

Remember that $\frac{d \sum g_{i}(x)}{dx} = \frac{\sum dg_{i}(x)}{dx}$. Therefore $\frac{\partial u^{2}}{\partial u} = 2u$ and $\frac{\partial u}{\partial \beta_{0}} = -1$, $\frac{\partial u}{\partial \beta_{1}} = -x_{i}$.
\end{frame}





\begin{frame}{Partial derivatives}
\begin{equation}
\frac{\partial u^{2}}{\partial \beta_{0}} = -2\sum (y_{i} - \beta_{0} - \beta_{1}x_{i})
\end{equation}

\begin{equation}
\frac{\partial u^{2}}{\partial \beta_{1}} = -2 x_{i} \sum (y_{i} - \beta_{0} - \beta_{1}x_{i})
\end{equation}

note that this will be zero if we know the exactly parameters.

\end{frame}


\begin{frame}{Gradient}
The gradient is the vector of partial derivates evaluated in a point $p$
\begin{equation}
\frac{\partial u^{2}}{\partial \bm{  \beta}} = \nabla (u^{2}) = \begin{pmatrix}
\frac{\partial(u^{2})}{\partial \beta_{0}} \\
\\
\frac{\partial(u^{2})}{\partial \beta_{1}}
\end{pmatrix}
\end{equation}

\end{frame}

\begin{frame}{Gradient descend}
in this equation $\alpha$ is the learning rate.

\begin{equation}
\bm{\beta_{i}} = \bm{\beta_{i-1}} - \alpha \nabla( u^{2}(\bm{\beta_{i-1}})) 
\end{equation}
\end{frame}


\begin{frame}[fragile]{python implementation}
\begin{lstlisting}
e
\end{lstlisting}
\end{frame}




\begin{frame}{Standardized coefficients}
Suppose a $\bm{X}$ vector (exogenous)  and $y$ (endogenous) that are transformed in $Z$ punctuation and for instance in the regression: $x_{1}$ have associated $\beta_{1}$.

The interpretation is:
\begin{block}{Interpreation}
The increase of one standard deviation in $x_{1}$ is associated with the increase (reduction) of $y$ in $\beta_{1}$ standard deviations. 
\end{block}
\end{frame}


\begin{frame}{Get standardized from OLS}
$\beta_{1}$ is a no-standardized coefficient,  and $\beta_{1std}$ is obtained from:
\begin{equation}
\beta_{1std} = \frac{\sigma_{x}}{\sigma_{y}}\beta_{1}
\end{equation}
where $\sigma_{x}$ and $\sigma_{y}$ are estimated standard deviations.
\end{frame}





\begin{frame}{Which have the major relative importance?}


\end{frame}




\begin{frame}{Statsmodels}

\end{frame}



\begin{frame}{Multivariate parameters estimation}

\end{frame}



\begin{frame}{Concepts needed}


\end{frame}

\begin{frame}{Inverse Matrix}
some
\end{frame}


\begin{frame}{Operations}

\end{frame}

\begin{frame}{Matrix differentiation}

\end{frame}





\begin{frame}{What is the result of correlation among variables?}
Testing in lab.

\end{frame}



\begin{frame}{Quantile regression}

\end{frame}


\end{document}