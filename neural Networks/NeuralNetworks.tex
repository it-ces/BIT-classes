\documentclass{beamer}
\usetheme{Madrid}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{nicematrix}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{array}
\usepackage{pgfcore}
\usepackage{listings}
\usepackage{color}
\definecolor{munsell}{rgb}{0.0, 0.5, 0.69}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{minas}{RGB}{0.244, 0.172, 0.36}
\definecolor{PUJ}{RGB}{44, 86, 151}
\definecolor{PUJ2}{RGB}{43, 93, 156}
\definecolor{PUJ3}{RGB}{20, 52, 107}
\definecolor{cyandk}{rgb}{0.0, 0.72, 0.92}
\setbeamerfont{frametitle}{size=\LARGE ,series=\bfseries}
\setbeamercolor{frametitle}{fg=PUJ2, bg=white} %% title of the beamer
\setbeamercolor{titlelike}
{parent=structure,bg=PUJ2}
\setbeamercolor{title}{fg=white, bg=PUJ3} 
%\setbeamercolor{navigation symbols}{fg=white, bg=white}
\setbeamercolor*{palette primary}{use=structure,fg=black,bg=yellow}
\setbeamercolor*{palette secondary}{use=structure,fg=white,bg=PUJ3}
\setbeamercolor*{palette tertiary}{use=structure,fg=white,bg=PUJ3}

\setbeamercolor{block title}{bg=PUJ3,fg=white}


%% code information
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  classoffset=1,
  morekeywords={True,False}, keywordstyle=\color{munsell}, 
  classoffset=0, 
  keywordstyle=\color{blue},  
  commentstyle=\color{dkgreen},
  stringstyle=\color{PUJ3},
  numberstyle=\tiny\color{gray},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4,
}

%% You can change default language in the middle of document with \lstset{language=Java}.


%% PUT or Remove the logo in a slide.


%% Information topic

\institute{Javeriana}
\date{2020}

\title[Pontificia Universidad Javeriana] %optional
{Introduction to Perceptron}
\subtitle{ using python.}

\author[Iván Andrés Trujillo] 
{
Iván Andrés Trujilllo Abella}

\institute[] 
{
  Facultad de Ingenieria\\
  Pontificia Universidad Javeriana
  \and
  
\textbf{ trujilloiv@javeriana.edu.co \\
addajaveriana}
}

\date[MINTA] % (optional)

\newif\ifplacelogo % create a new conditional
\placelogotrue % set it to true
%\logo{\ifplacelogo\color{red}\rule{.5cm}{.5cm}\fi}
\logo{\ifplacelogo \includegraphics[height= 2.0cm]{pujshield.eps}\fi}



\begin{document}



\frame{\titlepage}


\begin{frame}[fragile]{Background classification problem}
Fisher(1936) proposes the linear Discriminant, 
the problem consist in predict a binary outcome according to a set of features, for instance find the variables that could predict the bankruptcy.

The main objective is construct a $z = \mathbf{w^{t}x}$ 
score whose indicate the probability of belonging to a class.
\end{frame}


\begin{frame}{Binary classification problem}
$y_{0},y_{1} \in Y $ and $y_{0} \cup y_{1} = Y$ and $y_{0} \cap y_{1} = \emptyset$ both class are exhaustive and are defined without ambiguity. 
A vector of weights and a vector of features for a 
\[
w^{t} \mathbf{x} = 
\begin{bmatrix}
w_{1} \\
w_{2} \\
\vdots \\
\end{bmatrix}
\begin{bmatrix}
x_{1} & x_{2} & \cdots & x_{n}
\end{bmatrix}
= \sum_{i=1}^{n}w_{i}x_{i} = z_{i}
\]

\end{frame}

\begin{frame}[fragile]{Perceptron}
Rossenblant(1958)
We can define a vector input $\mathbf{x}$ and a vector of weights $\mathbf{w}$ and a activation function $\varphi$ that take as input the inner product of both vectors defined previously $\varphi(\mathbf{w}^{t}\mathbf{x})$.

\end{frame}



\begin{frame}
\tikzset{basic/.style={draw,fill=none,
                       text badly centered,minimum width=3em}}
\tikzset{input/.style={basic,circle,minimum width=3.5em}}
\tikzset{weights/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle, minimum width=4em}}
\newcommand{\addaxes}{\draw (0em,1em) -- (0em,-1em)
                            (-1em,0em) -- (1em,0em);}
\newcommand{\relu}{\draw[line width=1.5pt] (-1em,0) -- (0,0)
                                (0,0) -- (0.75em,0.75em);}
\newcommand{\stepfunc}{\draw[line width=1.5pt] (0.65em,0.65em) -- (0,0.65em) 
                                    -- (0,-0.65em) -- (-0.65em,-0.65em);}
    \begin{tikzpicture}[scale=1.2]
    % Draw input nodes
    \foreach \h [count=\hi ] in {$x_2$,$x_1$,$x_0=1$}{%
          \node[input] (f\hi) at (0,\hi*1.25cm-1.5 cm) {\h};
        }
    % Dot dot dot ... x_n
    \node[below=0.62cm] (idots) at (f1) {\vdots};
    \node[input, below=0.62cm] (last_input) at (idots) {$x_n$};
    % Draw summation node
    \node[functions] (sum) at (4,0) {\Huge$\sum$};
    \node[below=0.69cm] at (sum) {$z = \sum_{i=0}^n w_ix_i$};
    % Draw edges from input nodes to summation node
    \foreach \h [count=\hi ] in {$w_2$,$w_1$,$w_0$}{%
          \path (f\hi) -- node[weights] (w\hi) {\h} (sum);
          \draw[->] (f\hi) -- (w\hi);
          \draw[->] (w\hi) -- (sum);
        }
    % Dot dot dot ... w_n
    \node[below=0.05cm] (wdots) at (w1) {\vdots};
    \node[weights, below=0.45cm] (last_weight) at (wdots) {$w_n$};
    % Add edges for last node and last weight etc
    \path[draw,->] (last_input) -- (last_weight);
    \path[draw,->] (last_weight) -- (sum);
    % Draw node for activation function
    \node[functions] (activation) at (7,0) {};
    % Place activation function in its node
    \begin{scope}[xshift=7cm,scale=1.25]
        \addaxes
        % flexible selection of activation function
        \stepfunc
        % \stepfunc
    \end{scope}
    % Connect sum to relu
    \draw[->] (sum) -- (activation);
    \draw[->] (activation) -- ++(1,0);
    % Labels
    \node[above=1cm]  at (f3) {inputs};
    \node[above=1cm] at (w3) {weights};
    \node[above=1cm] at (activation) {activation function  $\varphi(z)$};
    \end{tikzpicture}
\end{frame}



\begin{frame}[fragile]{Activation function}
$\varphi()$ could be defined as the sigmoid function.

\begin{equation}
\begin{align*}
p(y=1) = \varphi(z) \\
p(y=0) = 1 -\varphi(z)
\end{align*}
\end{equation}
Percepton works with a step function:

\begin{equation*}
\varphi(z) = 
    \begin{dcases}
        0 &  z < 0 \\
        1 &  z \geq 0
    \end{dcases}
\end{equation*}    
Returning  a binary outcome.
\end{frame}


\begin{frame}{What set of $w$ values we must choose}{Cost function}
The cost function could be defined in a soft or a hard way.
\begin{equation}
J(w)_{hard} = \sum_{i=1}^{n} = \max(-y_{i}\hat{y},0)
\end{equation}
$J$ only count the number of mismatches.
However this function not is differentiable.

\begin{equation}
J(w)_{soft} = \sum_{i=1}^{n} = \max(-y_{i} z_{i},0)
\end{equation}
if $y_{i}z_{i} < 0 $  then lost function $>0$

if $y_{i}z_{i}>0 $  the lost function $=0$.

\end{frame}




\begin{frame}[fragile]{Insights}

The update of weights is according to the data bias or mistakes, however when the model match to the class then
\begin{equation}
\Delta w_{i} = (y_{i} -\hat{y_{i}}) = 0
\end{equation}
where $y_{i}$ is the real observed data, and $\hat{y_{i}}$ is the predicted class.

when the $y_{i} = -1$ and $\hat{y} = 1$ then $\Delta w = -2$, in otherwise $y_{i}=1$ and $\hat{y_{i}}=-1$ then $\Delta w = 2$.
in summary:

\begin{equation*}
\Delta w_{i} = 
    \begin{dcases}
        0 &  y_{i} = \hat{y_{i}} \\
       -2 &  y_{i}<\hat{y_{i}}  \\
         2 &  y_{i}>\hat{y_{i}}
    \end{dcases}
\end{equation*}    
\end{frame}


\begin{frame}{insights}
Then when there are mistakes 
\begin{equation}
\varphi(w^{t}_{i+1} \mathbf{x_{i}}) =\varphi((w_{i} + \Delta w_{i})^{t} \mathbf{ x_{i}}) = y_{i} 
\end{equation}
This mean that  weights for the vector of features of the sample $i$ are update to predict the correct class.
 
\begin{equation}
w_{i+1} = w_{i} + \eta \Delta w_{i} \mathbf{x_{i}}
\end{equation}

\end{frame}


\begin{frame}[fragile]{Perceptron algorithm}
\begin{verbatim}
initialize w:
for each x in sample :
	estimate y(x)
	w = w + update(w)
\end{verbatim}
\end{frame}

\begin{frame}{LAB}{Perceptron implementation from scratch}
\begin{Large}
\textbf{
\href{https://colab.research.google.com/drive/13yt7cav6wZbMrmwGxjxzPKwgo5636YUj?usp=sharing}{Perceptron from scratch(click here)}}
\end{Large}
\end{frame}



\begin{frame}{sklearn}
It is open source library, integrated with scipy and numpy. It is one of the most popular machine learning library on Github. 

\begin{itemize}
\item Classification  (Neural networks
 Support Vector Machine)
\item Decision trees
\item Cluster
\item Regression
\end{itemize} 

\end{frame}

\begin{frame}[fragile]{sklearn}
\begin{lstlisting}
from skelearn.linear_model import Perceptron
model = Perceptron(penalty=None , max_iter=1000, eta0=0.4, random_state=1)
model.fit(X,y)
model.score(X,y) # Print the number of matches
from sklearn.metrics import confusion_matrix
confusion_matrix(y, model.predict(X))
print(model.coef_)
\end{lstlisting}
\end{frame}


\begin{comment}


\begin{frame}{descent gradient}
An algorithm to optimization.
We can think in a greedy algorithm, searching points, for instance to fit a line to a data points, 
$e(\beta_{0}^{v}, \beta_{1}^{f})$
then we have an associeted error fo $\beta_{0}$ $e_{\beta_{0}^{v}} =\sum_{i=1}^{n} (y_{i} - \beta_{0}^{v}+ \beta_{1}^{f}x_{i} )^{2}$. 
\end{frame}


\begin{frame}[fragile]{Binary system}
A bit is a variable that could take two possible variables $0$ and $1$ therefore $n$ bits have $2^{n}$ possibles combinations.

Remember that $10$ is $2$, the number $100$  is $4$, the number $1010$ is $10$, the convertion of decimal system to binary system.
\end{frame}



\begin{frame}{ideas over information}
a certain event don yield information, a rare event yield more information, if two events are measured apart, the information yielded is the sum of two information separately.

\end{frame}



\begin{frame}[fragile]{Entropy}
a mesaure of not order. the uncertainty, for instance suppose that you flip a coin and maximun entropy is reached when the success have $p=\frac{1}{2}$ due we do not have additional information about what side lands.

\end{frame}


\begin{frame}[fragile]{Insights about entropy}
The event with small probability of occurrence give more information than those with a major probability.


\begin{itemize}
\item jensen inequality
\item Gibbs inequality
\end{itemize}

\end{frame}



\begin{frame}
\begin{equation}
\begin{align*}
I(x) &= log \frac{1}{p(x)}
     &= - log p(x)
\end{align*}
\end{equation}
the avarage surprisal:
\begin{equation}
\sum p(x)I(x)
\end{equation}
we can rewrite as:
\begin{equation}
H(x) = - \sum p(x) log p(x)
\end{equation}
suppose that we have a variable $x \sim N(\mu, \sigma)$
do not have more entropy than a variable $y \sim U(a,b)$ given that each 'character' have the same probability of occur.
\end{frame}


\begin{frame}{Entropy in Bernouilli trial}

\begin{equation}
H(x) - \sum_{k=1}^{2} \frac{1}{2} log_{2} \frac{1}{2} = 1
\end{equation}
Note that $p_{1}=1/2$ and $p_{2} = 1/2$.

\end{frame}



\begin{frame}{Conditional entropy}
\begin{equation}
H(X \mid Y ) = \sum_{i,j}p(x_{i},y_{j})log \frac{p(x_{i},y_{j})}{p(y_{j})}
\end{equation}
\end{frame}



\begin{frame}[fragile]{Gini coefficient}
A measure of dispersion, 
A measure of inequality based upon the lorentz curve, whose is a basic but smart idea, axis represent the cumulative proportion of the variable, in the cumulative entities( persons, regions, and so on).

\end{frame}



\begin{frame}{lorentz curve}
\definecolor{qqqqcc}{rgb}{0,0,0.8}
\definecolor{qqwuqq}{rgb}{0,0.39215686274509803,0}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45, scale=0.7]
\begin{axis}[
x=1cm,y=1cm,
axis lines=middle,
xmin=-0.1,
xmax=13.5,
ymin=0,
ymax=11,
xtick={0},
ytick={0},]
\clip(-0.8694140372315902,-4.314718926642073) rectangle (26.594349777827446,19.594337869801166);
\draw [line width=2pt,color=qqwuqq] (0,0)-- (13.16,11.1);
\draw [rotate around={-56.694479071423174:(-215.18175234556708,339.0826607244106)},line width=1.6pt,pattern color=qqqqcc] (-215.18175234556708,339.0826607244106) ellipse (402.68437368747675cm and 84.90521670471614cm);
\draw [line width=1.4pt,dotted] (13.080575941403652,11.028023396032836)-- (13,0);
\end{axis}
\end{tikzpicture}
\end{frame}


\begin{frame}[fragile]{Gini Coefficient}
Gini was defined as the area between  the line of perfect inequality and the lorentz curve  over the total area over the perfect inequality.
\begin{equation}
G = \frac{\int x dx - \int L(x)dx}{\int x dx}
\end{equation}
Note that the perfect equality line it is $y=x$ and we can said that $\int_{0}^{1} x dx =\frac{1}{2}$ due the are of triangle it is the half of the square with one dimension. Therefore:
\begin{equation}
G =  1 - 2 \int_{0}^{1} L(x) dx
\end{equation} 

the term equidistribution line.
\end{frame}




\begin{frame}{compute Gini index}
if we have $n$ observations then $Y = \sum_{j=1}^{n} y_{i}$, and assuming that $y_{1} \leq y_{2} ... \leq y_{n}$, the $l_{i} = \frac{\sum_{j=1}^{i} y_{j}}{Y}$  that is the cumulative income, equally the portion $x_{i} = \frac{i}{n}$.

then we search alternative calculate the gini index.
using one approximation we can define:
\begin{equation}
G = 1 - \sum (l_{i} + l_{i+1}) (x_{i+1} -x_{i})
\end{equation}
Notice that the trapecium are could be approximate as $((x_{i+1} - x_{i})l_{i} + (x_{i+1}-x_{i})l_{i+1})\frac{1}{2}$.
\end{frame}





\begin{frame}[fragile]{Python implementation I}
\begin{lstlisting}
def gini(array):
  # We must exclude 0 incomes?
  array.append(0)
  array = sorted(array)
  persons = len(array)-1
  acumulative_person = [x/persons for x in range(0,persons+1)]
  acumulative=[]
  count = 0
  for x in array:
    count = count + x
    acumulative.append(count)
  acumulative = [x/sum(array) for x in acumulative]
  
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]{Python implementation II}
\begin{lstlisting}
# Gini 
  areas = []
  for i in range(0,len(array)-1):
      areas.append((acumulative[i+1]+acumulative[i]) *
	(acumulative_person[i+1]-acumulative_person[i]))
  gini = 1-sum(areas)
  return acumulative , acumulative_person, gini
x,y, gini = gini(incomes)
plt.plot(y,x)
plt.plot(x,x)

\end{lstlisting}
\end{frame}




\begin{frame}[fragile]{example}
\begin{lstlisting}
from math import log2
log2(1/2)
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Theil index}
\begin{lstlisting}
from math import log
def theil(array):
  n = len(array)
  ymean = sum(array)/n
  data = [(x/ymean, log(x/ymean)) for x in array]
  Theil = sum([x[0]*x[1] for x in data])/n
  return Theil
\end{lstlisting}
\end{frame}




\begin{frame}[fragile]{Lost function}


\end{frame}



\begin{frame}{Research}
binary entropy function, log loss function.


\end{frame}


\begin{frame}
\begin{columns}
\column{0.5\textwidth}
\definecolor{qqqqcc}{rgb}{0,0,0.8}
\begin{tikzpicture}[line cap=round,line join=round, scale=0.8]
\clip(3.0110668914628027,-0.46146921060576085) rectangle (11.868629834326168,5.231022157124454);
\draw [shift={(3,1)},line width=0.8pt,color=qqqqcc,fill=qqqqcc,fill opacity=0.02] (0,0) -- (0:0.5796834386690682) arc (0:33.69006752597983:0.5796834386690682) -- cycle;
\draw [line width=2pt] (3,1)-- (9,5);
\draw [line width=2pt] (3,1)-- (9,1);
\draw [line width=2pt] (9,1)-- (9,5);
\draw [shift={(3,1)},->,line width=0.8pt,color=qqqqcc] (0:0.5796834386690682) arc (0:33.69006752597983:0.5796834386690682);
\draw (5.625439199860301,4) node[anchor=north west] {$\mathit{\mathbf{H}}$};
\draw (8.697761424806362,2.987647249475174) node[anchor=north west] {$\mathit{\mathbf{O}}$};
\draw (6.239903644849513,1) node[anchor=north west] {$\mathit{\mathbf{A}}$};
\begin{scriptsize}
\draw[color=qqqqcc] (3.7240775210257566,1.1413554973142028) node {$\alpha$};
\end{scriptsize}
\end{tikzpicture}

\column{0.5\textwidth}
\begin{equation}
sin = \frac{h}{o}
\end{equation}

\end{columns}
\end{frame}




\begin{frame}[fragile]{Directional derivate}


\end{frame}





\begin{comment}
shortfall = deficit
come out:

\end{comment}




\begin{frame}{Insights more deeply about perceptron}

The function $sing$ is defined as $\mathbb{R}
: \longrightarrow \{-1,1,0\}$


\end{frame}





\end{document}